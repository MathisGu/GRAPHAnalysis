{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns',None)\n",
    "import uproot\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import GravNetConv\n",
    "from torch_geometric.data import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_RUN THE FOLLOWING CELL ONLY IF THE HDF5 FILES HAVE NEVER BEEN CREATED, OTHERWISE SKIP THIS_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'root_pandas' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-0f1a34e35413>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;31m#dfTau_gentau = uproot.open(inFileTau_dict[FE])[treename].arrays()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m \u001b[0mdfTau_gentau\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroot_pandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_root\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minFileTau_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mFE\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtreename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbranches_event_cl3d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflatten\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;31m#dfTau_cl3d = uproot.open(inFileTau_dict[FE])[treename].tree.pandas.df(branches_cl3d, flatten=False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;31m#dfTau_tc = uproot.open(inFileTau_dict[FE])[treename].tree.pandas.df(branches_tc, flatten=False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'root_pandas' is not defined"
     ]
    }
   ],
   "source": [
    "indir = '/data_CMS_upgrade/motta/HGCAL_SKIMS/SKIM_18Feb2021'\n",
    "outdir = '/home/llr/cms/motta/HGCAL/CMSSW_11_1_0/src/GRAPHAnalysis/L1GNN/hdf5dataframes/skimLevel'\n",
    "os.system('mkdir -p '+outdir)\n",
    "\n",
    "FE = 'threshold'\n",
    "\n",
    "######################### IN-FILES ######################### \n",
    "\n",
    "inFileTau_dict = {\n",
    "    'threshold'    : indir+'/SKIM_RelValTenTau_PU200/mergedOutput.root',\n",
    "    'supertrigger' : indir+'/',\n",
    "    'bestchoice'   : indir+'/',\n",
    "    'bestcoarse'   : indir+'/',\n",
    "    'mixed'        : indir+'/'\n",
    "}\n",
    "\n",
    "inFileHH_dict = {\n",
    "    'threshold'    : indir+'/SKIM_GluGluHHTo2b2Tau_PU200/mergedOutput.root',\n",
    "    'supertrigger' : indir+'/',\n",
    "    'bestchoice'   : indir+'/',\n",
    "    'bestcoarse'   : indir+'/',\n",
    "    'mixed'        : indir+'/'\n",
    "}\n",
    "\n",
    "inFileNu_dict = {\n",
    "    'threshold'    : indir+'/SKIM_RelValNu_PU200/mergedOutput.root',\n",
    "    'supertrigger' : indir+'/',\n",
    "    'bestchoice'   : indir+'/',\n",
    "    'bestcoarse'   : indir+'/',\n",
    "    'mixed'        : indir+'/'\n",
    "}\n",
    "\n",
    "inFileQCD_dict = {\n",
    "    'threshold'    : indir+'/SKIM_QCD_PU200/mergedOutput.root',\n",
    "    'supertrigger' : indir+'/',\n",
    "    'bestchoice'   : indir+'/',\n",
    "    'bestcoarse'   : indir+'/',\n",
    "    'mixed'        : indir+'/'\n",
    "}\n",
    "\n",
    "\n",
    "######################### OUT-FILES ######################### \n",
    "\n",
    "outFileTau_dict = {\n",
    "    'threshold'    : outdir+'/RelValTenTau_PU200_th.hdf5',\n",
    "    'supertrigger' : outdir+'/',\n",
    "    'bestchoice'   : outdir+'/',\n",
    "    'bestcoarse'   : outdir+'/',\n",
    "    'mixed'        : outdir+'/'\n",
    "}\n",
    "\n",
    "outFileHH_dict = {\n",
    "    'threshold'    : outdir+'/GluGluHHTo2b2Tau_PU200_th.hdf5',\n",
    "    'supertrigger' : outdir+'/',\n",
    "    'bestchoice'   : outdir+'/',\n",
    "    'bestcoarse'   : outdir+'/',\n",
    "    'mixed'        : outdir+'/'\n",
    "}\n",
    "\n",
    "outFileNu_dict = {\n",
    "    'threshold'    : outdir+'/RelValNu_PU200_th.hdf5',\n",
    "    'supertrigger' : outdir+'/',\n",
    "    'bestchoice'   : outdir+'/',\n",
    "    'bestcoarse'   : outdir+'/',\n",
    "    'mixed'        : outdir+'/'\n",
    "}\n",
    "\n",
    "outFileQCD_dict = {\n",
    "    'threshold'    : outdir+'/QCD_PU200_th.hdf5',\n",
    "    'supertrigger' : outdir+'/',\n",
    "    'bestchoice'   : outdir+'/',\n",
    "    'bestcoarse'   : outdir+'/',\n",
    "    'mixed'        : outdir+'/'\n",
    "}\n",
    "\n",
    "\n",
    "######################### CREATE HDF5 FILES ######################### \n",
    "\n",
    "# TTree to be read\n",
    "treename = 'SkimmedTree'\n",
    "\n",
    "branches_tc = ['event', 'tc_id', 'tc_subdet', 'tc_zside', 'tc_layer', 'tc_waferu', 'tc_waferv', \n",
    "               'tc_wafertype', 'tc_panel_number', 'tc_panel_sector', 'tc_cellu', 'tc_cellv', 'tc_data', \n",
    "               'tc_uncompressedCharge', 'tc_compressedCharge',  'tc_pt', 'tc_energy', 'tc_eta', 'tc_phi', \n",
    "               'tc_x', 'tc_y', 'tc_z', 'tc_mipPt', 'tc_cluster_id', 'tc_multiuclaster_id', \n",
    "               'tc_multicluster_pt']\n",
    "\n",
    "branches_cl3d = ['event', 'cl3_id', 'cl3d_pt', 'cl3d_energy', 'cl3d_eta', 'cl3d_phi', 'cl3d_clusters_n',\n",
    "                 'cl3d_showerlength', 'cl3d_coreshowerlength',  'cl3d_firstlayer','cl3d_maxlayer','cl3d_seetot',\n",
    "                 'cl3d_spptot', 'cl3d_sppmax', 'cl3d_szz', 'cl3d_srrtot', 'cl3d_srrmax', 'cl3d_srrmean', \n",
    "                 'cled_emaxe', 'cl3d_hoe', 'cl3d_meanz', 'cl3d_layer10', 'cl3d_layer50', 'cl3d_layer90', \n",
    "                 'cl3d_ntc67', 'cl3d_ntc90', 'cl3d_bdteg', 'cl3d_uality']\n",
    "\n",
    "branches_gentau = ['event', 'gentau_pt', 'gentau_eta', 'gentau_phi', 'gentau_energy', 'gentau_mass', \n",
    "                   'gentau_vis_pt', 'gentau_vis_eta', 'gentau_vis_phi', 'gentau_vis_energy', 'gentau_vis_mass', \n",
    "                   'gentau_decayMode']\n",
    "\n",
    "branches_genjet = ['event', 'genjet_pt', 'genjet_eta', 'genjet_phi', 'genjet_energy', 'genjet_mass']\n",
    "\n",
    "\n",
    "#dfTau_gentau = uproot.open(inFileTau_dict[FE])[treename].arrays()\n",
    "dfTau_gentau = root_pandas.read_root(inFileTau_dict[FE], key=treename, columns=branches_event_cl3d, flatten=False)\n",
    "#dfTau_cl3d = uproot.open(inFileTau_dict[FE])[treename].tree.pandas.df(branches_cl3d, flatten=False)\n",
    "#dfTau_tc = uproot.open(inFileTau_dict[FE])[treename].tree.pandas.df(branches_tc, flatten=False)\n",
    "print(dfTau_gentau)\n",
    "\n",
    "'''\n",
    "#TBranches to be stored containing the 3D clusters' info\n",
    "branches_event_cl3d = ['event','cl3d_pt','cl3d_eta','cl3d_phi','cl3d_showerlength','cl3d_coreshowerlength', 'cl3d_firstlayer','cl3d_maxlayer','cl3d_seetot','cl3d_spptot','cl3d_szz', 'cl3d_srrtot', 'cl3d_srrmean', 'cl3d_hoe', 'cl3d_meanz', 'cl3d_layer10', 'cl3d_layer50', 'cl3d_layer90', 'cl3d_ntc67', 'cl3d_ntc90']\n",
    "branches_cl3d       = ['cl3d_pt','cl3d_eta','cl3d_phi','cl3d_showerlength','cl3d_coreshowerlength', 'cl3d_firstlayer','cl3d_maxlayer','cl3d_seetot','cl3d_spptot','cl3d_szz', 'cl3d_srrtot', 'cl3d_srrmean', 'cl3d_hoe', 'cl3d_meanz', 'cl3d_layer10', 'cl3d_layer50', 'cl3d_layer90', 'cl3d_ntc67', 'cl3d_ntc90']\n",
    "# TBranches to be stored containing the gen taus' info\n",
    "branches_event_gentau = ['event', 'gentau_pt', 'gentau_eta', 'gentau_phi', 'gentau_energy', 'gentau_mass', 'gentau_vis_pt', 'gentau_vis_eta', 'gentau_vis_phi', 'gentau_vis_energy', 'gentau_vis_mass', 'gentau_decayMode']\n",
    "branches_gentau       = ['gentau_pt', 'gentau_eta', 'gentau_phi', 'gentau_energy', 'gentau_mass', 'gentau_vis_pt', 'gentau_vis_eta', 'gentau_vis_phi', 'gentau_vis_energy', 'gentau_vis_mass', 'gentau_decayMode']\n",
    "# TBranches to be stored containing the gen jets' info\n",
    "branches_event_genjet = ['event', 'genjet_pt', 'genjet_eta', 'genjet_phi', 'genjet_energy', 'genjet_mass']\n",
    "branches_genjet       = ['genjet_pt', 'genjet_eta', 'genjet_phi', 'genjet_energy', 'genjet_mass']\n",
    "\n",
    "# fill tau dataframes and dictionaries -> training \n",
    "df_tau_cl3d = root_pandas.read_root(inFileTau_dict[FE], key=treename, columns=branches_event_cl3d, flatten=branches_cl3d)\n",
    "df_tau_gentau = root_pandas.read_root(inFileTau_dict[FE], key=treename, columns=branches_event_gentau, flatten=branches_gentau)\n",
    "dfTau = pd.concat([df_tau_cl3d,df_tau_gentau],sort=False)\n",
    "store_tau = pd.HDFStore(outFileTau_dict[FE], mode='w')\n",
    "store_tau[FE] = dfTau\n",
    "store_tau.close()\n",
    "\n",
    "# fill HH dataframes and dictionaries -> validation\n",
    "df_hh_cl3d = root_pandas.read_root(inFileHH_dict[FE], key=treename, columns=branches_event_cl3d, flatten=branches_cl3d)\n",
    "df_hh_gentau = root_pandas.read_root(inFileHH_dict[FE], key=treename, columns=branches_event_gentau, flatten=branches_gentau)\n",
    "dfHH = pd.concat([df_hh_cl3d,df_hh_gentau],sort=False)\n",
    "store_hh = pd.HDFStore(outFileHH_dict[FE], mode='w')\n",
    "store_hh[FE] = dfHH\n",
    "store_hh.close()\n",
    "\n",
    "# fill nu pileup dataframes and dictionaries\n",
    "df_nu_cl3d = root_pandas.read_root(inFileNu_dict[FE], key=treename, columns=branches_event_cl3d, flatten=branches_cl3d)\n",
    "dfNu = pd.concat([df_nu_cl3d],sort=False)\n",
    "store_nu = pd.HDFStore(outFileNu_dict[FE], mode='w')\n",
    "store_nu[FE] = dfNu\n",
    "store_nu.close()\n",
    "\n",
    "# fill QCD dataframes and dictionaries -> 1/2 training + 1/2 validation \n",
    "df_qcd_cl3d = root_pandas.read_root(inFileQCD_dict[FE], key=treename, columns=branches_event_cl3d, flatten=branches_cl3d)\n",
    "df_qcd_genjet = root_pandas.read_root(inFileQCD_dict[FE], key=treename, columns=branches_event_genjet, flatten=branches_genjet)\n",
    "dfQCD = pd.concat([df_qcd_cl3d,df_qcd_genjet],sort=False)\n",
    "dfQCD['gentau_decayMode'] = 4 # tag as QCD background\n",
    "store_qcd = pd.HDFStore(outFileQCD_dict[FE], mode='w')\n",
    "store_qcd[FE] = dfQCD\n",
    "store_qcd.close()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dfTau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_RUN FROM THE FOLLOWING CELL IF THE HDF5 FILES HAVE ALREADY BEEN CREATED_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indir = '/home/llr/cms/motta/HGCAL/CMSSW_11_1_0/src/GRAPHAnalysis/L1GNN/hdf5dataframes/skimLevel'\n",
    "\n",
    "FE = 'threshold'\n",
    "\n",
    "######################### IN-FILES ######################### \n",
    "\n",
    "inFileTau_dict = {\n",
    "    'threshold'    : indir+'/RelValTenTau_PU200_th.hdf5',\n",
    "    'supertrigger' : indir+'/',\n",
    "    'bestchoice'   : indir+'/',\n",
    "    'bestcoarse'   : indir+'/',\n",
    "    'mixed'        : indir+'/'\n",
    "}\n",
    "\n",
    "inFileHH_dict = {\n",
    "    'threshold'    : indir+'/GluGluHHTo2b2Tau_PU200_th.hdf5',\n",
    "    'supertrigger' : indir+'/',\n",
    "    'bestchoice'   : indir+'/',\n",
    "    'bestcoarse'   : indir+'/',\n",
    "    'mixed'        : indir+'/'\n",
    "}\n",
    "\n",
    "inFileNu_dict = {\n",
    "    'threshold'    : indir+'/RelValNu_PU200_th.hdf5',\n",
    "    'supertrigger' : indir+'/',\n",
    "    'bestchoice'   : indir+'/',\n",
    "    'bestcoarse'   : indir+'/',\n",
    "    'mixed'        : indir+'/'\n",
    "}\n",
    "\n",
    "inFileQCD_dict = {\n",
    "    'threshold'    : indir+'/QCD_PU200_th.hdf5',\n",
    "    'supertrigger' : indir+'/',\n",
    "    'bestchoice'   : indir+'/',\n",
    "    'bestcoarse'   : indir+'/',\n",
    "    'mixed'        : indir+'/'\n",
    "}\n",
    "\n",
    "# fill tau dataframes and dictionaries -> training \n",
    "store_tau = pd.HDFStore(inFileTau_dict[FE], mode='r')\n",
    "dfTau = store_tau[FE]\n",
    "store_tau.close()\n",
    "\n",
    "# fill nu pileup dataframes and dictionaries\n",
    "store_nu = pd.HDFStore(inFileNu_dict[FE], mode='r')\n",
    "dfNu = store_nu[FE]\n",
    "store_nu.close()   \n",
    "\n",
    "# fill HH dataframes and dictionaries -> validation\n",
    "store_hh = pd.HDFStore(inFileHH_dict[FE], mode='r')\n",
    "dfHH = store_hh[FE]\n",
    "store_hh.close()\n",
    "\n",
    "# fill QCD dataframes and dictionaries -> 1/2 training + 1/2 validation \n",
    "store_qcd= pd.HDFStore(inFileQCD_dict[FE], mode='r')\n",
    "dfQCD = store_qcd[FE]\n",
    "store_qcd.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bdtcut = 'cl3d_pubdt_passWP99'\n",
    "# bdtcut = 'cl3d_pubdt_passWP95'\n",
    "# bdtcut = 'cl3d_pubdt_passWP90'\n",
    "\n",
    "\n",
    "######################### SELECT EVENTS FOR TRAINING #########################  \n",
    "\n",
    "# SIGNAL\n",
    "dfTauTraining = dfTau.query('gentau_vis_pt>20' and 'gentau_vis_eta>1.6' and 'gentau_vis_eta<2.9' \n",
    "                            and 'gentau_vis_eta>-2.9' and 'gentau_vis_eta<-1.6' and 'cl3d_isbestmatch==True' \n",
    "                            and 'cl3d_pt>4' and 'gentau_decayMode>=0' and bdtcut+'==True')\n",
    "\n",
    "# BACKGROUND\n",
    "dfQCDTraining = dfQCD.sample(frac=0.5,random_state=10)\n",
    "dfQCDValidation = dfQCD.drop(dfQCDTraining.index)\n",
    "\n",
    "dfQCDTraining = dfQCDTraining.query('genjet_pt>20' and 'genjet_eta>1.6' and 'genjet_eta<2.9' \n",
    "                    and 'genjet_eta>-2.9' and 'genjet_eta<-1.6' and 'cl3d_isbestmatch==True' \n",
    "                    and 'cl3d_pt>4' and bdtcut+'==True')\n",
    "\n",
    "# VALIDATION\n",
    "dfHHValidation = dfHH.query('cl3d_eta>1.6' and 'cl3d_eta<2.9' and 'cl3d_pt_c3>-2.9' and 'cl3d_eta<-1.6' \n",
    "                            and 'cl3d_pt_c3>4')\n",
    "\n",
    "dfQCDValidation = dfQCDValidation.query('cl3d_eta>1.6' and 'cl3d_eta<2.9' and 'cl3d_pt_c3>-2.9' and 'cl3d_eta<-1.6' \n",
    "                            and 'cl3d_pt_c3>4')\n",
    "\n",
    "# MERGE\n",
    "dfMergedTraining = pd.concat([dfTauTraining,dfQCDTraining],sort=True)\n",
    "dfMergedValidation = pd.concat([dfHHValidation,dfQCDValidation],sort=False)\n",
    "\n",
    "dfMergedTraining = dfMergedTraining.astype('float64')\n",
    "dfMergedValidation = dfMergedValidation.astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### SELECT SEPARATE DMs #########################\n",
    "\n",
    "dfTau_DM0 = dfTau.query('gentau_decayMode==0')\n",
    "dfTau_DM1 = dfTau.query('gentau_decayMode==1')\n",
    "dfTau_DM10 = dfTau.query('gentau_decayMode==10')\n",
    "dfTau_DM11 = dfTau.query('gentau_decayMode==11')\n",
    "dfTau_DM5 = dfTau.query('gentau_decayMode==5')\n",
    "dfTau_DM2 = dfTau.query('gentau_decayMode==5' or 'gentau_decayMode==6')\n",
    "dfTau_DM3 = dfTau.query('gentau_decayMode==10' or 'gentau_decayMode==11')\n",
    "dfHH_DM0 = dfHH.query('gentau_decayMode==0')\n",
    "dfHH_DM1 = dfHH.query('gentau_decayMode==1')\n",
    "dfHH_DM10 = dfHH.query('gentau_decayMode==10')\n",
    "dfHH_DM11 = dfHH.query('gentau_decayMode==11')\n",
    "dfHH_DM5 = dfHH.query('gentau_decayMode==5')\n",
    "dfHH_DM2 = dfHH.query('gentau_decayMode==5' or 'gentau_decayMode==6')\n",
    "dfHH_DM3 = dfHH.query('gentau_decayMode==10' or 'gentau_decayMode==11')\n",
    "\n",
    "# replace DM>=5 with category numbers 2/3 (REMEMBER: category number 4 is QCD)\n",
    "dfMergedTraining['gentau_decayMode'].replace([5,6], 2, inplace=True)\n",
    "dfMergedValidation['gentau_decayMode'].replace([5,6], 2, inplace=True)\n",
    "dfMergedTraining['gentau_decayMode'].replace([10,11], 3, inplace=True)\n",
    "dfMergedValidation['gentau_decayMode'].replace([10,11], 3, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['cl3d_pt_c3', 'cl3d_abseta', 'cl3d_showerlength', 'cl3d_coreshowerlength', 'cl3d_firstlayer', \n",
    "            'cl3d_maxlayer', 'cl3d_szz', 'cl3d_seetot', 'cl3d_spptot', 'cl3d_srrtot', 'cl3d_srrmean', 'cl3d_hoe', \n",
    "            'cl3d_meanz', 'cl3d_layer10', 'cl3d_layer50', 'cl3d_layer90', 'cl3d_ntc67', 'cl3d_ntc90']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GravNetConv(18,6,4,3,4,aggr='mean')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrainingTensor = torch.tensor(dfMergedTraining[features].values)\n",
    "yTrainingTensor = torch.tensor(dfMergedTraining['gentau_decayMode'].values)\n",
    "data = Data(x=xTrainingTensor,y=yTrainingTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()  # Define loss criterion.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)  # Define optimizer.\n",
    "\n",
    "def train(data):\n",
    "    optimizer.zero_grad()  # Clear gradients.\n",
    "    out, h = model(data.x, data.y)   # Perform a single forward pass.\n",
    "    loss = criterion(out[data.train_mask], data.y[data.train_mask])  # Compute the loss solely based on the training nodes.\n",
    "    loss.backward()   # Derive gradients.\n",
    "    optimizer.step()  # Update parameters based on gradients.\n",
    "    return loss, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import KarateClub\n",
    "\n",
    "dataset = KarateClub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(dfMergedTraining[features].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
