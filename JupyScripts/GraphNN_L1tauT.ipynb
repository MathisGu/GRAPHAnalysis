{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIRST GraphNN BASED $\\tau$ L1 HGCAL TRIGGER "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jona Motta - March 2nd 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This L1 trgger is based on the studies carried out in [this paper](https://arxiv.org/pdf/1902.07987.pdf) by Shah Rukh Qasim, Jan Kieseler, Yutaro Iiyama, Maurizio Pierini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, auc\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following contains all the definition coming from the GraphNN study cited above and found [here](https://github.com/jkiesele/caloGraphNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### DEFINITIONS FROM caloGraphNN.py #########################  \n",
    "\n",
    "def gauss(x):\n",
    "    return tf.exp(-1* x*x)\n",
    "\n",
    "def gauss_of_lin(x):\n",
    "    return tf.exp(-1*(tf.abs(x)))\n",
    "\n",
    "def euclidean_squared(A, B):\n",
    "    \"\"\"\n",
    "    Returns euclidean distance between two batches of shape [B,N,F] and [B,M,F] where B is batch size, N is number of\n",
    "    examples in the batch of first set, M is number of examples in the batch of second set, F is number of spatial\n",
    "    features.\n",
    "\n",
    "    Returns:\n",
    "    A matrix of size [B, N, M] where each element [i,j] denotes euclidean distance between ith entry in first set and\n",
    "    jth in second set.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    shape_A = A.get_shape().as_list()\n",
    "    shape_B = B.get_shape().as_list()\n",
    "    \n",
    "    assert (A.dtype == tf.float32 or A.dtype == tf.float64) and (B.dtype == tf.float32 or B.dtype == tf.float64)\n",
    "    assert len(shape_A) == 3 and len(shape_B) == 3\n",
    "    assert shape_A[0] == shape_B[0]# and shape_A[1] == shape_B[1]\n",
    "\n",
    "    # Finds euclidean distance using property (a-b)^2 = a^2 + b^2 - 2ab\n",
    "    sub_factor = -2 * tf.matmul(A, tf.transpose(B, perm=[0, 2, 1]))  # -2ab term\n",
    "    dotA = tf.expand_dims(tf.reduce_sum(A * A, axis=2), axis=2)  # a^2 term\n",
    "    dotB = tf.expand_dims(tf.reduce_sum(B * B, axis=2), axis=1)  # b^2 term\n",
    "    return tf.abs(sub_factor + dotA + dotB)\n",
    "\n",
    "\n",
    "def nearest_neighbor_matrix(spatial_features, k=10):\n",
    "    \"\"\"\n",
    "    Nearest neighbors matrix given spatial features.\n",
    "\n",
    "    :param spatial_features: Spatial features of shape [B, N, S] where B = batch size, N = max examples in batch,\n",
    "                             S = spatial features\n",
    "    :param k: Max neighbors\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    shape = spatial_features.get_shape().as_list()\n",
    "\n",
    "    assert spatial_features.dtype == tf.float32 or spatial_features.dtype == tf.float64\n",
    "    assert len(shape) == 3\n",
    "\n",
    "    D = euclidean_squared(spatial_features, spatial_features)\n",
    "    D, N = tf.nn.top_k(-D, k)\n",
    "    return N, -D\n",
    "\n",
    "\n",
    "def indexing_tensor(spatial_features, k=10, n_batch=-1):\n",
    "\n",
    "    shape_spatial_features = spatial_features.get_shape().as_list()\n",
    "    n_batch = shape_spatial_features[0]\n",
    "    n_max_entries = shape_spatial_features[1]\n",
    "\n",
    "    # All of these tensors should be 3-dimensional\n",
    "    assert len(shape_spatial_features) == 3\n",
    "\n",
    "    # Neighbor matrix should be int as it should be used for indexing\n",
    "    assert spatial_features.dtype == tf.float64 or spatial_features.dtype == tf.float32\n",
    "\n",
    "    neighbor_matrix, distance_matrix = nearest_neighbor_matrix(spatial_features, k)\n",
    "\n",
    "    batch_range = tf.expand_dims(tf.expand_dims(tf.expand_dims(tf.range(0, n_batch), axis=1),axis=1), axis=1)\n",
    "    batch_range = tf.tile(batch_range, [1,n_max_entries,k,1])\n",
    "    expanded_neighbor_matrix = tf.expand_dims(neighbor_matrix, axis=3)\n",
    "    \n",
    "    \n",
    "    indexing_tensor = tf.concat([batch_range, expanded_neighbor_matrix], axis=3)\n",
    "\n",
    "    return tf.cast(indexing_tensor, tf.int64), distance_matrix\n",
    "\n",
    "\n",
    "#not really needed, maybe some performance advantage\n",
    "def high_dim_dense(inputs,nodes,**kwargs):\n",
    "    if len(inputs.shape) == 3:\n",
    "        return tf.layers.conv1d(inputs, nodes, kernel_size=(1), strides=(1), padding='valid', \n",
    "                                **kwargs)\n",
    "        \n",
    "    if len(inputs.shape) == 4:\n",
    "        return tf.layers.conv2d(inputs, nodes, kernel_size=(1,1), strides=(1,1), padding='valid', \n",
    "                                **kwargs)\n",
    "        \n",
    "    if len(inputs.shape) == 5:\n",
    "        return tf.layers.conv3d(inputs, nodes, kernel_size=(1,1,1), strides=(1,1,1), padding='valid', \n",
    "                                **kwargs)\n",
    "\n",
    "\n",
    "def apply_edges(vertices, edges, reduce_sum=True, flatten=True,expand_first_vertex_dim=True, aggregation_function=tf.reduce_max): \n",
    "    '''\n",
    "    edges are naturally BxVxV'xF\n",
    "    vertices are BxVxF'  or BxV'xF'\n",
    "    This function returns BxVxF'' if flattened and summed\n",
    "    '''\n",
    "    edges = tf.expand_dims(edges,axis=3)\n",
    "    if expand_first_vertex_dim:\n",
    "        vertices = tf.expand_dims(vertices,axis=1)\n",
    "    vertices = tf.expand_dims(vertices,axis=4)\n",
    "\n",
    "    out = edges*vertices # [BxVxV'x1xF] x [Bx1xV'xF'x1] = [BxVxV'xFxF']\n",
    "\n",
    "    if reduce_sum:\n",
    "        out = aggregation_function(out,axis=2)\n",
    "    if flatten:\n",
    "        out = tf.reshape(out,shape=[out.shape[0],out.shape[1],-1])\n",
    "    \n",
    "    return out\n",
    "\n",
    "\n",
    "### actual layers\n",
    "\n",
    "def layer_GarNet(vertices_in,\n",
    "                 n_aggregators,\n",
    "                 n_filters,\n",
    "                 n_propagate,\n",
    "                 plus_mean=True\n",
    "                 ):\n",
    "    \n",
    "    vertices_in_orig = vertices_in\n",
    "    vertices_in = tf.layers.dense(vertices_in,n_propagate,activation=None)\n",
    "    \n",
    "    agg_nodes = tf.layers.dense(vertices_in_orig,n_aggregators,activation=None) #BxVxNA, vertices_in: BxVxF\n",
    "    agg_nodes = gauss(agg_nodes)\n",
    "    vertices_in = tf.concat([vertices_in,agg_nodes], axis=-1)\n",
    "    \n",
    "    edges = tf.expand_dims(agg_nodes,axis=3) # BxVxNAx1\n",
    "    edges = tf.transpose(edges, perm=[0,2, 1,3]) # [BxVxV'xF]\n",
    "    \n",
    "    \n",
    "    vertices_in_collapsed = apply_edges(vertices_in, edges, reduce_sum=True, flatten=True)#,aggregation_function=tf.reduce_mean)# [BxNAxF]\n",
    "    vertices_in_mean_collapsed = apply_edges(vertices_in, edges, reduce_sum=True, flatten=True ,aggregation_function=tf.reduce_mean)# [BxNAxF]\n",
    "    \n",
    "    vertices_in_collapsed= tf.concat([vertices_in_collapsed,vertices_in_mean_collapsed],axis=-1 )\n",
    "    \n",
    "    edges = tf.transpose(edges, perm=[0,2, 1,3]) # [BxVxV'xF]\n",
    "    \n",
    "    expanded_collapsed = apply_edges(vertices_in_collapsed, edges, reduce_sum=False, flatten=True)# [BxVxF]\n",
    "    \n",
    "    expanded_collapsed = tf.concat([vertices_in_orig,expanded_collapsed,agg_nodes], axis=-1)\n",
    "    \n",
    "    merged_out = high_dim_dense(expanded_collapsed,n_filters,activation=tf.nn.tanh)\n",
    "    \n",
    "    return merged_out\n",
    "    \n",
    "    \n",
    "def layer_GravNet(vertices_in,\n",
    "                  n_neighbours,\n",
    "                  n_dimensions,\n",
    "                  n_filters,\n",
    "                  n_propagate):\n",
    "    \n",
    "    vertices_prop = high_dim_dense(vertices_in,n_propagate,activation=None)    \n",
    "    neighb_dimensions = high_dim_dense(vertices_in,n_dimensions,activation=None) #BxVxND, \n",
    "    \n",
    "    def collapse_to_vertex(indexing,distance,vertices):\n",
    "        neighbours = tf.gather_nd(vertices, indexing)  #BxVxNxF\n",
    "        distance = tf.expand_dims(distance,axis=3)\n",
    "        distance = distance*10. # input is tanh activated or batch normed, allow for some more spread\n",
    "        edges = gauss_of_lin(distance)[:,:,1:,:]\n",
    "        neighbours = neighbours[:,:,1:,:]\n",
    "        scaled_feat = edges*neighbours\n",
    "        collapsed = tf.reduce_max(scaled_feat, axis=2)\n",
    "        collapsed_mean = tf.reduce_mean(scaled_feat,axis=2)\n",
    "        collapsed = tf.concat([collapsed,collapsed_mean],axis=-1)\n",
    "        return collapsed\n",
    "    \n",
    "    indexing, distance = indexing_tensor(neighb_dimensions, n_neighbours)\n",
    "    collapsed = collapse_to_vertex(indexing,distance,vertices_prop)\n",
    "    updated_vertices = tf.concat([vertices_in,collapsed],axis=-1)\n",
    "\n",
    "    return high_dim_dense(updated_vertices,n_filters,activation=tf.nn.tanh)\n",
    "\n",
    "\n",
    "def layer_global_exchange(vertices_in):\n",
    "    trans_vertices_in = vertices_in\n",
    "\n",
    "    global_summed = tf.reduce_mean(trans_vertices_in, axis=1, keepdims=True)\n",
    "\n",
    "    global_summed = tf.tile(global_summed, [1, vertices_in.shape[1], 1])\n",
    "    vertices_out = tf.concat([vertices_in, global_summed], axis=-1)\n",
    "\n",
    "    return vertices_out\n",
    "\n",
    "\n",
    "\n",
    "######################### DEFINITIONS FROM tensorflow_models.py #########################\n",
    "\n",
    "def get_GravNet_model_for_clustering(input, training, momentum):\n",
    "    feats = []\n",
    "    x = input\n",
    "    for i in range(4):\n",
    "        x = layer_global_exchange(x)\n",
    "        x = high_dim_dense(x, 64, activation=tf.nn.tanh)\n",
    "        x = high_dim_dense(x, 64, activation=tf.nn.tanh)\n",
    "        x = high_dim_dense(x, 64, activation=tf.nn.tanh)\n",
    "\n",
    "        x = layer_GravNet(x,\n",
    "                          n_neighbours=40,\n",
    "                          n_dimensions=4,\n",
    "                          n_filters=42,\n",
    "                          n_propagate=18)\n",
    "        x = tf.layers.batch_normalization(x, momentum=momentum, training=training)\n",
    "        feats.append(x)\n",
    "\n",
    "    x = tf.concat(feats, axis=-1)\n",
    "    x = high_dim_dense(x, 128, activation=tf.nn.relu)\n",
    "    x = high_dim_dense(x, 3, activation=tf.nn.relu)\n",
    "    return x\n",
    "\n",
    "def get_GarNet_model_for_clustering(input, training, momentum):\n",
    "    aggregators = 11 * [4]\n",
    "    filters = 11 * [32]\n",
    "    propagate = 11 * [20]\n",
    "\n",
    "    feat = layer_global_exchange(input)\n",
    "    feat = tf.layers.batch_normalization(feat, training=training, momentum=momentum)\n",
    "    feat = high_dim_dense(feat, 32, activation=tf.nn.tanh)\n",
    "    feat_list = []\n",
    "    for i in range(len(filters)):\n",
    "        feat = layer_GarNet(feat,\n",
    "                            aggregators[i],\n",
    "                            n_filters=filters[i],\n",
    "                            n_propagate=propagate[i]\n",
    "                            )\n",
    "        feat = tf.layers.batch_normalization(feat, training=training, momentum=momentum)\n",
    "        feat_list.append(feat)\n",
    "        # feat = tf.layers.dropout(feat, rate=0.0005, training=self.is_train)\n",
    "\n",
    "    feat = tf.concat(feat_list, axis=-1)\n",
    "    feat = tf.layers.dense(feat, 48, activation=tf.nn.relu)\n",
    "    feat = tf.layers.dense(feat, 3, activation=tf.nn.relu)\n",
    "\n",
    "    return feat\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
